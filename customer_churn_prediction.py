# -*- coding: utf-8 -*-
"""Customer churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFb-fES4YKJESK46GIxKPt-ZlmsCWYm_

# Proyek pertama: Telecom customer churn
- **Nama:** Halim Sajidi
- **Email:** halimsajidi14@gmail.com
- **ID Dicoding:** halimsajidi
- **Link:** https://www.kaggle.com/datasets/shilongzhuang/telecom-customer-churn-by-maven-analytics?resource=download

## Library
"""

pip install optuna

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
import optuna
import lightgbm as lgb

"""## Data Wrangling

### Gathering Data
"""

df = pd.read_csv('telecom_customer_churn.csv')

data = pd.read_csv('telecom_customer_churn.csv')

df

"""Dataset telecom customer churn memiliki 38 kolom dan 7043 baris

### Assessing Data
"""

df.info()

df.describe()

df.isnull().sum()

print("Jumlah duplikasi: ", df.duplicated().sum())

"""**select categorical data**"""

df_cat = df.select_dtypes(include="object")
cat_col = df_cat.columns
df_cat.head(5)

"""**Select numerical data**"""

df_num = df.select_dtypes(include=["int64", "float64"])
num_col = df_num.columns
df_num

"""## Menangani Missing Value dan Outlier

### Missing Value

**Numerikal missing value**

membersihkan nan value pada setiap kolom numerik
"""

df[num_col].isna().sum()

for column in num_col:
  if df[column].isnull().any():
    df[column].fillna(df[column].mean(), inplace=True)

df[num_col].isna().sum()

"""**Kategorikal missing value**

membersihkan nan value pada setiap kolom numerik
"""

df[cat_col].isna().sum()

for column in cat_col:
    if df[column].isnull().any():
        mode_value = df[column].mode()[0]
        df[column].fillna(mode_value, inplace=True)

df[cat_col].isna().sum()

"""### Outlier"""

plt.figure(figsize=(20, 12))

# Calculate the number of rows and columns needed for the subplots
num_rows = 4
num_cols = 4

for i, column in enumerate(num_col, 1):
    plt.subplot(num_rows, num_cols, i)
    sns.boxplot(data=df, x=df[column])
    plt.tight_layout()

Q1 = df[num_col].quantile(0.25)
Q3 = df[num_col].quantile(0.75)
IQR = Q3 - Q1

# Create a mask for outliers
outliers_mask = (df[num_col] < (Q1 - 1.5 * IQR)) | (df[num_col] > (Q3 + 1.5 * IQR))

# Replace outliers with the mean of each column
for column in num_col:
    mean_value = df[column].mean()
    df.loc[outliers_mask[column], column] = mean_value

df_clean = df.copy()

"""Bagian ini didapatkan outlier pada dataset dan solusi yang dilakukan adalah dengan mengubah outlier tersebut menjadi nan yang kemudian nilai nan akan diisi dengan rata-rata (mean)"""

df_clean

"""## Exploratory Data Analysis (EDA)

### Univariate Analysis
"""

sns.countplot(data=df_clean, x='Gender')
plt.title('Distribution of Gender')
plt.show()

df_clean['Payment Method'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Payment Method Distribution')
plt.ylabel('')
plt.show()

df_clean.groupby(['Gender', 'Churn Category']).size().unstack().plot(kind='bar', stacked=True)
plt.title('Churn Category by Gender')
plt.ylabel('Count')
plt.show()

contingency_table = pd.crosstab(df_clean['Internet Service'], df_clean['Churn Category'])
sns.heatmap(contingency_table, annot=True, fmt='d')
plt.title('Internet Service vs Churn Category')
plt.show()

sns.countplot(data=df_clean, x='Internet Type')
plt.title('Distribution of Internet Type')
plt.xticks(rotation=0)
plt.show()

"""### Multivariate

**Pairplot**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Buat pairplot untuk kolom numerik
sns.pairplot(df_clean[num_col], diag_kind='kde')
plt.suptitle('Pairplot for Numeric Features', y=1.02)
plt.show()

"""**Heatmap**"""

# Hitung korelasi antar kolom numerik
corr_matrix = df_clean[num_col].corr()

# Buat heatmap
plt.figure(figsize=(15, 12))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Heatmap of Numerical Features Correlation')
plt.show()

"""**countplot**"""

# Buat countplot untuk distribusi 'Customer Status' berdasarkan 'Churn Category'
plt.figure(figsize=(10, 6))
sns.countplot(data=df_clean, x='Customer Status', hue='Churn Category')
plt.title('Customer Status vs Churn Category')
plt.show()

"""**boxplot**"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_clean, x='Customer Status', y='Monthly Charge')
plt.title('Monthly Charge Distribution by Customer Status')
plt.show()

"""## Data Preparation

### Encoding Data
"""

le = LabelEncoder()

for column in cat_col:
    df_clean[column] = le.fit_transform(df_clean[column])

df_clean

"""### splitting data"""

X = df_clean.drop(columns=['Customer ID','Customer Status'])
y = df_clean['Customer Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('X_train', X_train.shape)  #Printing the X_train Features Shape
print('y_train', y_train.shape)  #Printing the y_train Labels Shape
print('X_test', X_test.shape)    #Printing the X_test Features Shape
print('y_test', y_test.shape)    #Printing the y_test Labels  Shape

"""## Machine Learning Model

### LGBM
"""

# Initialize the LGBM Classifier
lgbm_model = LGBMClassifier(random_state=42)

# Train the model
lgbm_model.fit(X_train, y_train)

# Predicting the X_test
pred = lgbm_model.predict(X_test)

# Testing the model accuracy with different parameters
acc = accuracy_score(y_test, pred)  # Getting the Accuracy Score
f1 = f1_score(y_test, pred, average='macro')  # Getting the F1 Score for multiclass
rec = recall_score(y_test, pred, average='macro')  # Getting the Recall Score for multiclass
prec = precision_score(y_test, pred, average='macro')  # Getting the Precision Score for multiclass
cm_lgbm = confusion_matrix(y_test, pred)  # Getting the confusion Matrix

# Defining a DataFrame
results = pd.DataFrame([['LGBM', acc, f1, rec, prec]],
                           columns=['Model', 'Accuracy_Score', 'F1_Score', 'Recall_Score', 'Precision_Score'])

# Printing the Model Results with different Metrics
results

plt.figure(figsize=(10, 7))
sns.heatmap(cm_lgbm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""### Random Forest"""

#Building the model
clf_rf = RandomForestClassifier(random_state=42)#Creating the Random Forest Classifier Model
clf_rf.fit(X_train, y_train)                   #Training the Model with X_train & y_train

# Membuat prediksi pada X_test
pred = clf_rf.predict(X_test)

# Menghitung metrik evaluasi
acc = accuracy_score(y_test, pred)  # Menghitung Accuracy
f1 = f1_score(y_test, pred, average='macro')  # Menghitung F1 Score (multiclass)
rec = recall_score(y_test, pred, average='macro')  # Menghitung Recall (multiclass)
prec = precision_score(y_test, pred, average='macro')  # Menghitung Precision (multiclass)
cm_rf = confusion_matrix(y_test, pred)  # Menghitung Confusion Matrix

# Menyimpan hasil evaluasi dalam DataFrame
model_results = pd.DataFrame([['Random Forest', acc, f1, rec, prec]],
                             columns=['Model', 'Accuracy_Score', 'F1_Score', 'Recall_Score', 'Precision_Score'])

# Menggabungkan hasil evaluasi ke dalam DataFrame `results`
results = pd.concat([results, model_results], ignore_index=True)

# Menampilkan hasil
results

plt.figure(figsize=(10, 7))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""### XGBoost"""

# Building the model
clf_xgb = XGBClassifier(random_state=42)
clf_xgb.fit(X_train, y_train)

# Membuat prediksi pada X_test
pred = clf_xgb.predict(X_test)

# Menghitung metrik evaluasi
acc = accuracy_score(y_test, pred)  # Menghitung Accuracy
f1 = f1_score(y_test, pred, average='macro')  # Menghitung F1 Score (multiclass)
rec = recall_score(y_test, pred, average='macro')  # Menghitung Recall (multiclass)
prec = precision_score(y_test, pred, average='macro')  # Menghitung Precision (multiclass)
cm_xgb = confusion_matrix(y_test, pred)  # Menghitung Confusion Matrix

# Menyimpan hasil evaluasi dalam DataFrame
model_results = pd.DataFrame([['XGBoost', acc, f1, rec, prec]],
                             columns=['Model', 'Accuracy_Score', 'F1_Score', 'Recall_Score', 'Precision_Score'])

# Menggabungkan hasil evaluasi ke dalam DataFrame `results`
results = pd.concat([results, model_results], ignore_index=True)

# Menampilkan hasil
results

plt.figure(figsize=(10, 7))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""### SVM"""

# Building the model
clf_svm = SVC(random_state=42)
clf_svm.fit(X_train, y_train)

# Making predictions on X_test
pred = clf_svm.predict(X_test)

# Calculating evaluation metrics
acc = accuracy_score(y_test, pred)  # Calculating Accuracy
f1 = f1_score(y_test, pred, average='macro')  # Calculating F1 Score (multiclass)
rec = recall_score(y_test, pred, average='macro')  # Calculating Recall (multiclass)
prec = precision_score(y_test, pred, average='macro')  # Calculating Precision (multiclass)
cm_svm = confusion_matrix(y_test, pred)  # Calculating Confusion Matrix

# Storing evaluation results in a DataFrame
model_results = pd.DataFrame([['SVM', acc, f1, rec, prec]],
                             columns=['Model', 'Accuracy_Score', 'F1_Score', 'Recall_Score', 'Precision_Score'])

# Combining evaluation results into the `results` DataFrame
results = pd.concat([results, model_results], ignore_index=True)

# Displaying the results
results

plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""## Hyperparameter

Berdasarkan basemodel yang sudah didapatkan, model lgbm menjadi yang paling akurat sehingga model ini dipilih untuk dilakukan hyperparameter dengan optuna
"""

# Fungsi objective untuk Optuna
def objective(trial):
    # Membuat hyperparameter yang akan dioptimalkan oleh Optuna
    param = {
        'objective': 'multiclass',
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'num_class': len(np.unique(y)),  # Jumlah kelas dalam target
        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)
    }

    # Membuat model LightGBM
    model = lgb.LGBMClassifier(**param)

    # Melatih model
    model.fit(X_train, y_train)

    # Membuat prediksi pada validation set
    y_pred = model.predict(X_test)

    # Menghitung akurasi
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

# Membuat study untuk optimasi
study = optuna.create_study(direction='maximize')  # Arahkan untuk memaksimalkan akurasi
study.optimize(objective, n_trials=100)  # Cobalah 50 percobaan (bisa disesuaikan)

# Menampilkan hasil terbaik
print("Best hyperparameters: ", study.best_params)

